{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Decision_Tree.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wOrOJct5Lgcx",
        "colab_type": "text"
      },
      "source": [
        "**Mike Donahoe**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NGnqFNp_Lmb-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#### Setting up dataset\n",
        "zoo_csv = \"\"\"aardvark,1,0,0,1,0,0,1,1,1,1,0,0,4,0,0,1,1\n",
        "antelope,1,0,0,1,0,0,0,1,1,1,0,0,4,1,0,1,1\n",
        "bass,0,0,1,0,0,1,1,1,1,0,0,1,0,1,0,0,4\n",
        "bear,1,0,0,1,0,0,1,1,1,1,0,0,4,0,0,1,1\n",
        "boar,1,0,0,1,0,0,1,1,1,1,0,0,4,1,0,1,1\n",
        "buffalo,1,0,0,1,0,0,0,1,1,1,0,0,4,1,0,1,1\n",
        "calf,1,0,0,1,0,0,0,1,1,1,0,0,4,1,1,1,1\n",
        "carp,0,0,1,0,0,1,0,1,1,0,0,1,0,1,1,0,4\n",
        "catfish,0,0,1,0,0,1,1,1,1,0,0,1,0,1,0,0,4\n",
        "cavy,1,0,0,1,0,0,0,1,1,1,0,0,4,0,1,0,1\n",
        "cheetah,1,0,0,1,0,0,1,1,1,1,0,0,4,1,0,1,1\n",
        "chicken,0,1,1,0,1,0,0,0,1,1,0,0,2,1,1,0,2\n",
        "chub,0,0,1,0,0,1,1,1,1,0,0,1,0,1,0,0,4\n",
        "clam,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,7\n",
        "crab,0,0,1,0,0,1,1,0,0,0,0,0,4,0,0,0,7\n",
        "crayfish,0,0,1,0,0,1,1,0,0,0,0,0,6,0,0,0,7\n",
        "crow,0,1,1,0,1,0,1,0,1,1,0,0,2,1,0,0,2\n",
        "deer,1,0,0,1,0,0,0,1,1,1,0,0,4,1,0,1,1\n",
        "dogfish,0,0,1,0,0,1,1,1,1,0,0,1,0,1,0,1,4\n",
        "dolphin,0,0,0,1,0,1,1,1,1,1,0,1,0,1,0,1,1\n",
        "dove,0,1,1,0,1,0,0,0,1,1,0,0,2,1,1,0,2\n",
        "duck,0,1,1,0,1,1,0,0,1,1,0,0,2,1,0,0,2\n",
        "elephant,1,0,0,1,0,0,0,1,1,1,0,0,4,1,0,1,1\n",
        "flamingo,0,1,1,0,1,0,0,0,1,1,0,0,2,1,0,1,2\n",
        "flea,0,0,1,0,0,0,0,0,0,1,0,0,6,0,0,0,6\n",
        "frog,0,0,1,0,0,1,1,1,1,1,0,0,4,0,0,0,5\n",
        "frog,0,0,1,0,0,1,1,1,1,1,1,0,4,0,0,0,5\n",
        "fruitbat,1,0,0,1,1,0,0,1,1,1,0,0,2,1,0,0,1\n",
        "giraffe,1,0,0,1,0,0,0,1,1,1,0,0,4,1,0,1,1\n",
        "girl,1,0,0,1,0,0,1,1,1,1,0,0,2,0,1,1,1\n",
        "gnat,0,0,1,0,1,0,0,0,0,1,0,0,6,0,0,0,6\n",
        "goat,1,0,0,1,0,0,0,1,1,1,0,0,4,1,1,1,1\n",
        "gorilla,1,0,0,1,0,0,0,1,1,1,0,0,2,0,0,1,1\n",
        "gull,0,1,1,0,1,1,1,0,1,1,0,0,2,1,0,0,2\n",
        "haddock,0,0,1,0,0,1,0,1,1,0,0,1,0,1,0,0,4\n",
        "hamster,1,0,0,1,0,0,0,1,1,1,0,0,4,1,1,0,1\n",
        "hare,1,0,0,1,0,0,0,1,1,1,0,0,4,1,0,0,1\n",
        "hawk,0,1,1,0,1,0,1,0,1,1,0,0,2,1,0,0,2\n",
        "herring,0,0,1,0,0,1,1,1,1,0,0,1,0,1,0,0,4\n",
        "honeybee,1,0,1,0,1,0,0,0,0,1,1,0,6,0,1,0,6\n",
        "housefly,1,0,1,0,1,0,0,0,0,1,0,0,6,0,0,0,6\n",
        "kiwi,0,1,1,0,0,0,1,0,1,1,0,0,2,1,0,0,2\n",
        "ladybird,0,0,1,0,1,0,1,0,0,1,0,0,6,0,0,0,6\n",
        "lark,0,1,1,0,1,0,0,0,1,1,0,0,2,1,0,0,2\n",
        "leopard,1,0,0,1,0,0,1,1,1,1,0,0,4,1,0,1,1\n",
        "lion,1,0,0,1,0,0,1,1,1,1,0,0,4,1,0,1,1\n",
        "lobster,0,0,1,0,0,1,1,0,0,0,0,0,6,0,0,0,7\n",
        "lynx,1,0,0,1,0,0,1,1,1,1,0,0,4,1,0,1,1\n",
        "mink,1,0,0,1,0,1,1,1,1,1,0,0,4,1,0,1,1\n",
        "mole,1,0,0,1,0,0,1,1,1,1,0,0,4,1,0,0,1\n",
        "mongoose,1,0,0,1,0,0,1,1,1,1,0,0,4,1,0,1,1\n",
        "moth,1,0,1,0,1,0,0,0,0,1,0,0,6,0,0,0,6\n",
        "newt,0,0,1,0,0,1,1,1,1,1,0,0,4,1,0,0,5\n",
        "octopus,0,0,1,0,0,1,1,0,0,0,0,0,8,0,0,1,7\n",
        "opossum,1,0,0,1,0,0,1,1,1,1,0,0,4,1,0,0,1\n",
        "oryx,1,0,0,1,0,0,0,1,1,1,0,0,4,1,0,1,1\n",
        "ostrich,0,1,1,0,0,0,0,0,1,1,0,0,2,1,0,1,2\n",
        "parakeet,0,1,1,0,1,0,0,0,1,1,0,0,2,1,1,0,2\n",
        "penguin,0,1,1,0,0,1,1,0,1,1,0,0,2,1,0,1,2\n",
        "pheasant,0,1,1,0,1,0,0,0,1,1,0,0,2,1,0,0,2\n",
        "pike,0,0,1,0,0,1,1,1,1,0,0,1,0,1,0,1,4\n",
        "piranha,0,0,1,0,0,1,1,1,1,0,0,1,0,1,0,0,4\n",
        "pitviper,0,0,1,0,0,0,1,1,1,1,1,0,0,1,0,0,3\n",
        "platypus,1,0,1,1,0,1,1,0,1,1,0,0,4,1,0,1,1\n",
        "polecat,1,0,0,1,0,0,1,1,1,1,0,0,4,1,0,1,1\n",
        "pony,1,0,0,1,0,0,0,1,1,1,0,0,4,1,1,1,1\n",
        "porpoise,0,0,0,1,0,1,1,1,1,1,0,1,0,1,0,1,1\n",
        "puma,1,0,0,1,0,0,1,1,1,1,0,0,4,1,0,1,1\n",
        "pussycat,1,0,0,1,0,0,1,1,1,1,0,0,4,1,1,1,1\n",
        "raccoon,1,0,0,1,0,0,1,1,1,1,0,0,4,1,0,1,1\n",
        "reindeer,1,0,0,1,0,0,0,1,1,1,0,0,4,1,1,1,1\n",
        "rhea,0,1,1,0,0,0,1,0,1,1,0,0,2,1,0,1,2\n",
        "scorpion,0,0,0,0,0,0,1,0,0,1,1,0,8,1,0,0,7\n",
        "seahorse,0,0,1,0,0,1,0,1,1,0,0,1,0,1,0,0,4\n",
        "seal,1,0,0,1,0,1,1,1,1,1,0,1,0,0,0,1,1\n",
        "sealion,1,0,0,1,0,1,1,1,1,1,0,1,2,1,0,1,1\n",
        "seasnake,0,0,0,0,0,1,1,1,1,0,1,0,0,1,0,0,3\n",
        "seawasp,0,0,1,0,0,1,1,0,0,0,1,0,0,0,0,0,7\n",
        "skimmer,0,1,1,0,1,1,1,0,1,1,0,0,2,1,0,0,2\n",
        "skua,0,1,1,0,1,1,1,0,1,1,0,0,2,1,0,0,2\n",
        "slowworm,0,0,1,0,0,0,1,1,1,1,0,0,0,1,0,0,3\n",
        "slug,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,7\n",
        "sole,0,0,1,0,0,1,0,1,1,0,0,1,0,1,0,0,4\n",
        "sparrow,0,1,1,0,1,0,0,0,1,1,0,0,2,1,0,0,2\n",
        "squirrel,1,0,0,1,0,0,0,1,1,1,0,0,2,1,0,0,1\n",
        "starfish,0,0,1,0,0,1,1,0,0,0,0,0,5,0,0,0,7\n",
        "stingray,0,0,1,0,0,1,1,1,1,0,1,1,0,1,0,1,4\n",
        "swan,0,1,1,0,1,1,0,0,1,1,0,0,2,1,0,1,2\n",
        "termite,0,0,1,0,0,0,0,0,0,1,0,0,6,0,0,0,6\n",
        "toad,0,0,1,0,0,1,0,1,1,1,0,0,4,0,0,0,5\n",
        "tortoise,0,0,1,0,0,0,0,0,1,1,0,0,4,1,0,1,3\n",
        "tuatara,0,0,1,0,0,0,1,1,1,1,0,0,4,1,0,0,3\n",
        "tuna,0,0,1,0,0,1,1,1,1,0,0,1,0,1,0,1,4\n",
        "vampire,1,0,0,1,1,0,0,1,1,1,0,0,2,1,0,0,1\n",
        "vole,1,0,0,1,0,0,0,1,1,1,0,0,4,1,0,0,1\n",
        "vulture,0,1,1,0,1,0,1,0,1,1,0,0,2,1,0,1,2\n",
        "wallaby,1,0,0,1,0,0,0,1,1,1,0,0,2,1,0,1,1\n",
        "wasp,1,0,1,0,1,0,0,0,0,1,1,0,6,0,0,0,6\n",
        "wolf,1,0,0,1,0,0,1,1,1,1,0,0,4,1,0,1,1\n",
        "worm,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,7\n",
        "wren,0,1,1,0,1,0,0,0,1,1,0,0,2,1,0,0,2\n",
        "\"\"\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rcF4koc8MTK3",
        "colab_type": "code",
        "outputId": "a21bdcdc-7781-437e-f64e-08eaf3bf9bfe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        }
      },
      "source": [
        "\"\"\"\n",
        "Make the imports of python packages needed\n",
        "\"\"\"\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pprint import pprint\n",
        "\n",
        "\n",
        "#### Loading dataset into pandas\n",
        "dataset = pd.DataFrame([x.split(',') for x in zoo_csv.split('\\n')[0:]], columns=['animal_name','hair','feathers','eggs','milk',\n",
        "                                                   'airbone','aquatic','predator','toothed','backbone',\n",
        "                                                  'breathes','venomous','fins','legs','tail','domestic','catsize','class',])\n",
        "\n",
        "#We drop the animal names since this is not a good feature to split the data on\n",
        "dataset=dataset.drop('animal_name',axis=1)\n",
        "\n",
        "print(dataset)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "     hair feathers  eggs  milk airbone  ...  legs  tail domestic catsize class\n",
            "0       1        0     0     1       0  ...     4     0        0       1     1\n",
            "1       1        0     0     1       0  ...     4     1        0       1     1\n",
            "2       0        0     1     0       0  ...     0     1        0       0     4\n",
            "3       1        0     0     1       0  ...     4     0        0       1     1\n",
            "4       1        0     0     1       0  ...     4     1        0       1     1\n",
            "..    ...      ...   ...   ...     ...  ...   ...   ...      ...     ...   ...\n",
            "97      1        0     1     0       1  ...     6     0        0       0     6\n",
            "98      1        0     0     1       0  ...     4     1        0       1     1\n",
            "99      0        0     1     0       0  ...     0     0        0       0     7\n",
            "100     0        1     1     0       1  ...     2     1        0       0     2\n",
            "101  None     None  None  None    None  ...  None  None     None    None  None\n",
            "\n",
            "[102 rows x 17 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gTDzCvKxMmV5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def entropy(target_col):\n",
        "    \"\"\"\n",
        "    Calculate the entropy of a dataset.\n",
        "    The only parameter of this function is the target_col parameter which specifies the target column\n",
        "    \"\"\"\n",
        "    elements,counts = np.unique(target_col,return_counts = True)\n",
        "    entropy = np.sum([(-counts[i]/np.sum(counts))*np.log2(counts[i]/np.sum(counts)) for i in range(len(elements))])\n",
        "    return entropy\n",
        "\n",
        "def InfoGain(data,split_attribute_name,target_name=\"class\"):\n",
        "    \"\"\"\n",
        "    Calculate the information gain of a dataset. This function takes three parameters:\n",
        "    1. data = The dataset for whose feature the IG should be calculated\n",
        "    2. split_attribute_name = the name of the feature for which the information gain should be calculated\n",
        "    3. target_name = the name of the target feature. The default for this example is \"class\"\n",
        "    \"\"\"    \n",
        "    #Calculate the entropy of the total dataset\n",
        "    total_entropy = entropy(data[target_name])\n",
        "    \n",
        "    ##Calculate the entropy of the dataset\n",
        "    \n",
        "    #Calculate the values and the corresponding counts for the split attribute \n",
        "    vals,counts= np.unique(data[split_attribute_name],return_counts=True)\n",
        "    \n",
        "    #Calculate the weighted entropy\n",
        "    Weighted_Entropy = np.sum([(counts[i]/np.sum(counts))*entropy(data.where(data[split_attribute_name]==vals[i]).dropna()[target_name]) for i in range(len(vals))])\n",
        "    \n",
        "    #Calculate the information gain\n",
        "    Information_Gain = total_entropy - Weighted_Entropy\n",
        "    return Information_Gain\n",
        "\n",
        "def Error(target_col):\n",
        "  probs = []\n",
        "  elements, counts = np.unique(target_col, return_counts=True)\n",
        "  probs.append((counts[i]/np.sum(counts))for i in range(len(elements)))\n",
        "  error = np.argmax(probs)\n",
        "  return 1- error\n",
        "\n",
        "def Weighted_Error(data, split_attribute_name, target_name=\"class\"):\n",
        "  vals,counts= np.unique(data[split_attribute_name],return_counts=True)\n",
        "  error = np.sum([(counts[i]/np.sum(counts))*Error(data.where(data[split_attribute_name]==vals[i]).dropna()[target_name]) for i in range(len(vals))])\n",
        "  return error\n",
        "\n",
        "def GiniIndex(target_col):\n",
        "  elements, counts = np.unique(target_col, return_counts = True)\n",
        "  Gini = 1-np.sum([(counts[i]/np.sum(counts))**2 for i in range(len(elements))])\n",
        "  return Gini\n",
        "\n",
        "def InfoGainGini(data, split_attribute_name, target_name=\"class\"):\n",
        "    ParentGini = GiniIndex(data[target_name])\n",
        "    vals,counts= np.unique(data[split_attribute_name],return_counts=True)\n",
        "    Weighted_Gini = np.sum([(counts[i]/np.sum(counts))*GiniIndex(data.where(data[split_attribute_name]==vals[i]).dropna()[target_name]) for i in range(len(vals))])\n",
        "    Information_Gain = ParentGini-Weighted_Gini\n",
        "    return Information_Gain\n",
        "  \n",
        "def Information_Gain_Ratio(data, split_attribute_name, target_name=\"class\"):\n",
        "    return InfoGain(data, split_attribute_name)/entropy(data[target_name])\n",
        "\n",
        "\n",
        "def ID3(data,originaldata,features,target_attribute_name=\"class\",parent_node_class = None, measurement = 'entropy'):\n",
        "    \"\"\"\n",
        "    ID3 Algorithm: This function takes five paramters:\n",
        "    1. data = the data for which the ID3 algorithm should be run --> In the first run this equals the total dataset\n",
        " \n",
        "    2. originaldata = This is the original dataset needed to calculate the mode target feature value of the original dataset\n",
        "    in the case the dataset delivered by the first parameter is empty\n",
        "\n",
        "    3. features = the feature space of the dataset . This is needed for the recursive call since during the tree growing process\n",
        "    we have to remove features from our dataset --> Splitting at each node\n",
        "\n",
        "    4. target_attribute_name = the name of the target attribute\n",
        "\n",
        "    5. parent_node_class = This is the value or class of the mode target feature value of the parent node for a specific node. This is \n",
        "    also needed for the recursive call since if the splitting leads to a situation that there are no more features left in the feature\n",
        "    space, we want to return the mode target feature value of the direct parent node.\n",
        "\n",
        "    6. measurement = entropy, gini, IGR, or error\n",
        "    \"\"\"   \n",
        "    #Define the stopping criteria --> If one of this is satisfied, we want to return a leaf node#\n",
        "    \n",
        "    #If all target_values have the same value, return this value\n",
        "    if len(np.unique(data[target_attribute_name])) <= 1:\n",
        "        return np.unique(data[target_attribute_name])[0]\n",
        "    \n",
        "    #If the dataset is empty, return the mode target feature value in the original dataset\n",
        "    elif len(data)==0:\n",
        "        return np.unique(originaldata[target_attribute_name])[np.argmax(np.unique(originaldata[target_attribute_name],return_counts=True)[1])]\n",
        "    \n",
        "    #If the feature space is empty, return the mode target feature value of the direct parent node --> Note that\n",
        "    #the direct parent node is that node which has called the current run of the ID3 algorithm and hence\n",
        "    #the mode target feature value is stored in the parent_node_class variable.\n",
        "    \n",
        "    elif len(features) ==0:\n",
        "        return parent_node_class\n",
        "    \n",
        "    #If none of the above holds true, grow the tree!\n",
        "    \n",
        "    else:\n",
        "        #Set the default value for this node --> The mode target feature value of the current node\n",
        "        parent_node_class = np.unique(data[target_attribute_name])[np.argmax(np.unique(data[target_attribute_name],return_counts=True)[1])]\n",
        "        \n",
        "        #Select the feature which best splits the dataset\n",
        "        if measurement=='entropy':\n",
        "          item_values = [InfoGain(data,feature,target_attribute_name) for feature in features] #Return the information gain values for the features in the dataset\n",
        "          best_feature_index = np.argmax(item_values)\n",
        "          best_feature = features[best_feature_index]\n",
        "        elif measurement == 'gini':\n",
        "          item_values = [InfoGainGini(data,feature,target_attribute_name) for feature in features] #Return the information gain values for the features in the dataset\n",
        "          best_feature_index = np.argmax(item_values)\n",
        "          best_feature = features[best_feature_index]\n",
        "        elif measurement == 'IGR':\n",
        "          item_values = [Information_Gain_Ratio(data,feature,target_attribute_name) for feature in features] #Return the information gain values for the features in the dataset\n",
        "          best_feature_index = np.argmax(item_values)\n",
        "          best_feature = features[best_feature_index]\n",
        "        else:\n",
        "          item_values = [Weighted_Error(data,feature,target_attribute_name) for feature in features] #Return the information gain values for the features in the dataset\n",
        "          best_feature_index = np.argmax(item_values)\n",
        "          best_feature = features[best_feature_index]\n",
        "        #Create the tree structure. The root gets the name of the feature (best_feature) with the maximum information\n",
        "        #gain in the first run\n",
        "        tree = {best_feature:{}}\n",
        "        \n",
        "        \n",
        "        #Remove the feature with the best inforamtion gain from the feature space\n",
        "        features = [i for i in features if i != best_feature]\n",
        "        \n",
        "        #Grow a branch under the root node for each possible value of the root node feature\n",
        "        \n",
        "        for value in np.unique(data[best_feature]):\n",
        "            value = value\n",
        "            #Split the dataset along the value of the feature with the largest information gain and therwith create sub_datasets\n",
        "            sub_data = data.where(data[best_feature] == value).dropna()\n",
        "            \n",
        "            #Call the ID3 algorithm for each of those sub_datasets with the new parameters --> Here the recursion comes in!\n",
        "            subtree = ID3(sub_data,dataset,features,target_attribute_name,parent_node_class,measurement)\n",
        "            \n",
        "            #Add the sub tree, grown from the sub_dataset to the tree under the root node\n",
        "            tree[best_feature][value] = subtree\n",
        "\n",
        "        return(tree)    \n",
        "                \n",
        "###################\n",
        "\n",
        "###################\n",
        "\n",
        "\n",
        "    \n",
        "    \n",
        "def predict(query,tree,default = 1):\n",
        "    \"\"\"\n",
        "    Prediction of a new/unseen query instance. This takes two parameters:\n",
        "    1. The query instance as a dictionary of the shape {\"feature_name\":feature_value,...}\n",
        "\n",
        "    2. The tree \n",
        "\n",
        "\n",
        "    We do this also in a recursive manner. That is, we wander down the tree and check if we have reached a leaf or if we are still in a sub tree. \n",
        "    Since this is a important step to understand, the single steps are extensively commented below.\n",
        "\n",
        "    1.Check for every feature in the query instance if this feature is existing in the tree.keys() for the first call, \n",
        "    tree.keys() only contains the value for the root node \n",
        "    --> if this value is not existing, we can not make a prediction and have to \n",
        "    return the default value which is the majority value of the target feature\n",
        "\n",
        "    2. First of all we have to take care of a important fact: Since we train our model with a database A and then show our model\n",
        "    a unseen query it may happen that the feature values of these query are not existing in our tree model because non of the\n",
        "    training instances has had such a value for this specific feature. \n",
        "    For instance imagine the situation where your model has only seen animals with one to four\n",
        "    legs - The \"legs\" node in your model will only have four outgoing branches (from one to four). If you now show your model\n",
        "    a new instance (animal) which has for the legs feature the vale 5, you have to tell your model what to do in such a \n",
        "    situation because otherwise there is no classification possible because in the classification step you try to \n",
        "    run down the outgoing branch with the value 5 but there is no such a branch. Hence: Error and no Classification!\n",
        "    We can address this issue with a classification value of for instance (999) which tells us that there is no classification\n",
        "    possible or we assign the most frequent target feature value of our dataset used to train the model. Or, in for instance \n",
        "    medical application we can return the most worse case - just to make sure... \n",
        "    We can also return the most frequent value of the direct parent node. To make a long story short, we have to tell the model \n",
        "    what to do in this situation.\n",
        "    In our example, since we are dealing with animal species where a false classification is not that critical, we will assign\n",
        "    the value 1 which is the value for the mammal species (for convenience).\n",
        "\n",
        "    3. Address the key in the tree which fits the value for key --> Note that key == the features in the query. \n",
        "    Because we want the tree to predict the value which is hidden under the key value (imagine you have a drawn tree model on \n",
        "    the table in front of you and you have a query instance for which you want to predict the target feature \n",
        "    - What are you doing? - Correct:\n",
        "    You start at the root node and wander down the tree comparing your query to the node values. Hence you want to have the\n",
        "    value which is hidden under the current node. If this is a leaf, perfect, otherwise you wander the tree deeper until you\n",
        "    get to a leaf node. \n",
        "    Though, you want to have this \"something\" [either leaf or sub_tree] which is hidden under the current node\n",
        "    and hence we must address the node in the tree which == the key value from our query instance. \n",
        "    This is done with tree[keys]. Next you want to run down the branch of this node which is equal to the value given \"behind\"\n",
        "    the key value of your query instance e.g. if you find \"legs\" == to tree.keys() that is, for the first run == the root node.\n",
        "    You want to run deeper and therefore you have to address the branch at your node whose value is == to the value behind key.\n",
        "    This is done with query[key] e.g. query[key] == query['legs'] == 0 --> Therewith we run down the branch of the node with the\n",
        "    value 0. Summarized, in this step we want to address the node which is hidden behind a specific branch of the root node (in the first run)\n",
        "    this is done with: result = [key][query[key]]\n",
        "\n",
        "    4. As said in the 2. step, we run down the tree along nodes and branches until we get to a leaf node.\n",
        "    That is, if result = tree[key][query[key]] returns another tree object (we have represented this by a dict object --> \n",
        "    that is if result is a dict object) we know that we have not arrived at a root node and have to run deeper the tree. \n",
        "    Okay... Look at your drawn tree in front of you... what are you doing?...well, you run down the next branch... \n",
        "    exactly as we have done it above with the slight difference that we already have passed a node and therewith \n",
        "    have to run only a fraction of the tree --> You clever guy! That \"fraction of the tree\" is exactly what we have stored\n",
        "    under 'result'.\n",
        "    So we simply call our predict method using the same query instance (we do not have to drop any features from the query\n",
        "    instance since for instance the feature for the root node will not be available in any of the deeper sub_trees and hence \n",
        "    we will simply not find that feature) as well as the \"reduced / sub_tree\" stored in result.\n",
        "\n",
        "    SUMMARIZED: If we have a query instance consisting of values for features, we take this features and check if the \n",
        "    name of the root node is equal to one of the query features.\n",
        "    If this is true, we run down the root node outgoing branch whose value equals the value of query feature == the root node.\n",
        "    If we find at the end of this branch a leaf node (not a dict object) we return this value (this is our prediction).\n",
        "    If we instead find another node (== sub_tree == dict objct) we search in our query for the feature which equals the value \n",
        "    of that node. Next we look up the value of our query feature and run down the branch whose value is equal to the \n",
        "    query[key] == query feature value. And as you can see this is exactly the recursion we talked about\n",
        "    with the important fact that for each node we run down the tree, we check only the nodes and branches which are \n",
        "    below this node and do not run the whole tree beginning at the root node \n",
        "    --> This is why we re-call the classification function with 'result'\n",
        "    \"\"\"\n",
        "    \n",
        "    \n",
        "    #1.\n",
        "    for key in list(query.keys()):\n",
        "        if key in list(tree.keys()):\n",
        "            #2.\n",
        "            try:\n",
        "                result = tree[key][query[key]] \n",
        "            except:\n",
        "                return default\n",
        "  \n",
        "            #3.\n",
        "            result = tree[key][query[key]]\n",
        "            #4.\n",
        "            if isinstance(result,dict):\n",
        "                return predict(query,result)\n",
        "\n",
        "            else:\n",
        "                return result\n",
        "\n",
        "        \n",
        "        \n",
        "\"\"\"\n",
        "Check the accuracy of our prediction.\n",
        "The train_test_split function takes the dataset as parameter which should be divided into\n",
        "a training and a testing set. The test function takes two parameters, which are the testing data as well as the tree model.\n",
        "\"\"\"\n",
        "###################\n",
        "\n",
        "###################\n",
        "\n",
        "def train_test_split(dataset):\n",
        "    training_data = dataset.iloc[:80].reset_index(drop=True)#We drop the index respectively relabel the index\n",
        "    #starting form 0, because we do not want to run into errors regarding the row labels / indexes\n",
        "    testing_data = dataset.iloc[80:].reset_index(drop=True)\n",
        "    return training_data,testing_data\n",
        "\n",
        "\n",
        "def test(data,tree):\n",
        "    #Create new query instances by simply removing the target feature column from the original dataset and \n",
        "    #convert it to a dictionary\n",
        "    queries = data.iloc[:,:-1].to_dict(orient = \"records\")\n",
        "    \n",
        "    #Create a empty DataFrame in whose columns the prediction of the tree are stored\n",
        "    predicted = pd.DataFrame(columns=[\"predicted\"]) \n",
        "    \n",
        "    #Calculate the prediction accuracy\n",
        "    for i in range(len(data)):\n",
        "        predicted.loc[i,\"predicted\"] = predict(queries[i],tree,1.0) \n",
        "    print('The prediction accuracy is: ',(np.sum(predicted[\"predicted\"] == data[\"class\"])/len(data))*100,'%')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zQCNj3mPMbww",
        "colab_type": "code",
        "outputId": "c5715250-d301-449d-c7c6-2fed339d36f7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 442
        }
      },
      "source": [
        "training_data = train_test_split(dataset)[0]\n",
        "testing_data = train_test_split(dataset)[1] \n",
        "\n",
        "\n",
        "\"\"\"\n",
        "Train the tree, Print the tree and predict the accuracy\n",
        "\"\"\"\n",
        "\"\"\"\n",
        "Test Entropy\n",
        "\"\"\"\n",
        "\n",
        "tree = ID3(training_data,training_data,training_data.columns[:-1],\"class\",None, 'entropy')\n",
        "print('entropy')\n",
        "pprint(tree)\n",
        "test(testing_data,tree)\n",
        "\"\"\"\n",
        "Test Gini\n",
        "\"\"\"\n",
        "tree = ID3(training_data,training_data,training_data.columns[:-1],\"class\",None,'gini')\n",
        "print('gini')\n",
        "pprint(tree)\n",
        "test(testing_data,tree)\n",
        "\"\"\"\n",
        "Test Information Gain Ratio\n",
        "\"\"\"\n",
        "tree = ID3(training_data,training_data,training_data.columns[:-1],\"class\",None,'IGR')\n",
        "print('IGR')\n",
        "pprint(tree)\n",
        "test(testing_data,tree)\n",
        "\"\"\"\n",
        "Test Error\n",
        "\"\"\"\n",
        "tree = ID3(training_data,training_data,training_data.columns[:-1],\"class\",None,'error')\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "entropy\n",
            "{'legs': {'0': {'fins': {'0': {'toothed': {'0': '7', '1': '3'}},\n",
            "                         '1': {'eggs': {'0': '1', '1': '4'}}}},\n",
            "          '2': {'hair': {'0': '2', '1': '1'}},\n",
            "          '4': {'hair': {'0': {'toothed': {'0': '7', '1': '5'}}, '1': '1'}},\n",
            "          '6': {'aquatic': {'0': '6', '1': '7'}},\n",
            "          '8': '7'}}\n",
            "The prediction accuracy is:  81.81818181818183 %\n",
            "gini\n",
            "{'legs': {'0': {'milk': {'0': {'fins': {'0': {'toothed': {'0': '7', '1': '3'}},\n",
            "                                        '1': '4'}},\n",
            "                         '1': '1'}},\n",
            "          '2': {'hair': {'0': '2', '1': '1'}},\n",
            "          '4': {'hair': {'0': {'toothed': {'0': '7', '1': '5'}}, '1': '1'}},\n",
            "          '6': {'aquatic': {'0': '6', '1': '7'}},\n",
            "          '8': '7'}}\n",
            "The prediction accuracy is:  81.81818181818183 %\n",
            "IGR\n",
            "{'legs': {'0': {'fins': {'0': {'toothed': {'0': '7', '1': '3'}},\n",
            "                         '1': {'eggs': {'0': '1', '1': '4'}}}},\n",
            "          '2': {'hair': {'0': '2', '1': '1'}},\n",
            "          '4': {'hair': {'0': {'toothed': {'0': '7', '1': '5'}}, '1': '1'}},\n",
            "          '6': {'aquatic': {'0': '6', '1': '7'}},\n",
            "          '8': '7'}}\n",
            "The prediction accuracy is:  81.81818181818183 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bglMXSzYcbYe",
        "colab_type": "code",
        "outputId": "5b0bfb2f-1e67-49db-87d8-f0a0d6a6f7b3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 663
        }
      },
      "source": [
        "###### Result I: Show the final accuracy of test dataset using four different measurements\n",
        "\"\"\"\n",
        "Test Entropy\n",
        "\"\"\"\n",
        "\n",
        "tree = ID3(training_data,training_data,training_data.columns[:-1],\"class\",None, 'entropy')\n",
        "pprint(tree)\n",
        "test(testing_data,tree)\n",
        "\"\"\"\n",
        "Test Gini\n",
        "\"\"\"\n",
        "tree = ID3(training_data,training_data,training_data.columns[:-1],\"class\",None,'gini')\n",
        "pprint(tree)\n",
        "test(testing_data,tree)\n",
        "\"\"\"\n",
        "Test Information Gain Ratio\n",
        "\"\"\"\n",
        "tree = ID3(training_data,training_data,training_data.columns[:-1],\"class\",None,'IGR')\n",
        "pprint(tree)\n",
        "test(testing_data,tree)\n",
        "\"\"\"\n",
        "Test Error\n",
        "\"\"\"\n",
        "tree = ID3(training_data,training_data,training_data.columns[:-1],\"class\",None,'error')\n",
        "pprint(tree)\n",
        "test(testing_data,tree)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'legs': {'0': {'fins': {'0': {'toothed': {'0': '7', '1': '3'}},\n",
            "                         '1': {'eggs': {'0': '1', '1': '4'}}}},\n",
            "          '2': {'hair': {'0': '2', '1': '1'}},\n",
            "          '4': {'hair': {'0': {'toothed': {'0': '7', '1': '5'}}, '1': '1'}},\n",
            "          '6': {'aquatic': {'0': '6', '1': '7'}},\n",
            "          '8': '7'}}\n",
            "The prediction accuracy is:  81.81818181818183 %\n",
            "{'legs': {'0': {'milk': {'0': {'fins': {'0': {'toothed': {'0': '7', '1': '3'}},\n",
            "                                        '1': '4'}},\n",
            "                         '1': '1'}},\n",
            "          '2': {'hair': {'0': '2', '1': '1'}},\n",
            "          '4': {'hair': {'0': {'toothed': {'0': '7', '1': '5'}}, '1': '1'}},\n",
            "          '6': {'aquatic': {'0': '6', '1': '7'}},\n",
            "          '8': '7'}}\n",
            "The prediction accuracy is:  81.81818181818183 %\n",
            "{'legs': {'0': {'fins': {'0': {'toothed': {'0': '7', '1': '3'}},\n",
            "                         '1': {'eggs': {'0': '1', '1': '4'}}}},\n",
            "          '2': {'hair': {'0': '2', '1': '1'}},\n",
            "          '4': {'hair': {'0': {'toothed': {'0': '7', '1': '5'}}, '1': '1'}},\n",
            "          '6': {'aquatic': {'0': '6', '1': '7'}},\n",
            "          '8': '7'}}\n",
            "The prediction accuracy is:  81.81818181818183 %\n",
            "{'hair': {'0': {'feathers': {'0': {'eggs': {'0': {'milk': {'0': {'airbone': {'0': {'aquatic': {'0': '7',\n",
            "                                                                                               '1': '3'}}}},\n",
            "                                                           '1': '1'}},\n",
            "                                            '1': {'milk': {'0': {'airbone': {'0': {'aquatic': {'0': {'predator': {'0': '6',\n",
            "                                                                                                                  '1': {'toothed': {'0': '7',\n",
            "                                                                                                                                    '1': '3'}}}},\n",
            "                                                                                               '1': {'predator': {'0': '4',\n",
            "                                                                                                                  '1': {'toothed': {'0': '7',\n",
            "                                                                                                                                    '1': {'backbone': {'1': {'breathes': {'0': '4',\n",
            "                                                                                                                                                                          '1': '5'}}}}}}}}}},\n",
            "                                                                             '1': '6'}}}}}},\n",
            "                             '1': '2'}},\n",
            "          '1': {'feathers': {'0': {'eggs': {'0': '1',\n",
            "                                            '1': {'milk': {'0': '6',\n",
            "                                                           '1': '1'}}}}}}}}\n",
            "The prediction accuracy is:  77.27272727272727 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c3pPUG4vcd-l",
        "colab_type": "code",
        "outputId": "1bb4ac3f-66de-4692-eeed-3f95ab76a60c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 367
        }
      },
      "source": [
        "###### Result II: Show the accuracy of test dataset using sklearn.tree.DecisionTreeClassifier\n",
        "from sklearn import tree\n",
        "data = np.array(training_data,training_data.columns[:-1])\n",
        "testdata= np.array(testing_data[:(len(testing_data)-1)],testing_data.columns[:-1])\n",
        "labels=[]\n",
        "for x in testdata:\n",
        "  labels.append(x[len(x)-1])\n",
        "clf = tree.DecisionTreeClassifier()\n",
        "clf = clf.fit(data,training_data['class'])\n",
        "pprint(clf)\n",
        "clf.predict(testdata)\n",
        "tree.plot_tree(clf)\n",
        "print(clf.score(testdata,labels))\n",
        "#Decision tree classifier had a better accuracy than the implemented  tree classifiers."
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n",
            "                       max_depth=None, max_features=None, max_leaf_nodes=None,\n",
            "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
            "                       min_samples_leaf=1, min_samples_split=2,\n",
            "                       min_weight_fraction_leaf=0.0, presort='deprecated',\n",
            "                       random_state=None, splitter='best')\n",
            "1.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAV0AAADnCAYAAAC9roUQAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}